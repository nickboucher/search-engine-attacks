from genericpath import exists
import numpy as np
import string
import os
import random
from scipy import spatial
from transformers import pipeline
from transformers import AutoTokenizer, AutoModel
import torch
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import json
import datetime
from datasets import load_dataset
from nltk import word_tokenize
from sentence_transformers import SentenceTransformer
from nltk.tokenize.treebank import TreebankWordDetokenizer


random.seed(123)

import warnings
# warnings.filterwarnings("ignore")


###----define the adversarial class -----------------
class adversarial_summarizer():
    def __init__(self, model):
        """
            model: the summarizer istance.
        """
        #save the settings
        self.model = model
        self.alphabet = string.ascii_letters
        self.deletion = chr(0x8)
        # self.deletion = [chr(0x8), chr(0x7F), chr(0xD)]

    def sanitize(self, x):
        """ Function that correctly sanitize the input"""
        x = list(x)
        x = [xx for xx in x if xx != self.deletion]
        x = ''.join(x)
        return x

    def summarize(self, batch):
        """
            Given a list of sentences [batch], it returns a list of summatizations.
        """
        #forward the model
        x = self.model(batch, truncation=True, do_sample=False)

        #unpack the results
        x = [xx['summary_text'] for xx in x]

        return x

    def score_function(self, y_true_summ, batch_cand):
        #empty vector containing the results
        res = []

        # sanitize the candidates from possible unicode characters
        # that might affect the scoring function
        batch_cand = [self.sanitize(x) for x in batch_cand]

        for cand in batch_cand:
            # print(y_true_summ, "\n" ,cand, "\n\n\n")
            score = sentence_bleu([y_true_summ.split()], cand.split(), smoothing_function=SmoothingFunction().method1)
            res.append(score)

        return res

    def poison(self, x, step = 1):
        """ This function generates the maximum perturbation of a given sample"""
        x = word_tokenize(x)
        res = []
        for i, word in enumerate(x):
            word = list(word) #convert to characters
            # print(word)

            if len(word) == 1:
                res.append(word[0])
            else:
                idx = round(len(word) / 2)  #get the index of insertion
                # print(idx)
                # w = word[0: idx] + random.sample(self.deletion, 1) + word[idx: ]
                w = word[0: idx] + [self.deletion] + word[idx: ]
                # w = word[0: idx] + ["XxX"] + word[idx: ]

                w_ = ''.join(w) #back to string

                res.append(w_)

        x = ' '.join(res)

        return x

    def count_perturbations(self, sentence):
        """ Count the size of the perturbation"""
        size = len([c for c in list(sentence) if c == self.deletion])
        perc = (size / len(list(sentence))) * size
        return size, perc

    def get_perturbated_idx(self, sentence):
        """ return a list of perturbation indexes"""
        return [i for i, c in enumerate(list(sentence)) if c == self.deletion]


    def population_generation(self, starting, max_population = 5, step = 50):
        """ Given a candidate, the function generates a population derived from it.
            In this case the population is generated by sanitizing the input by N steps.
        """
        #get a list of perturbated characters indexes
        idx = self.get_perturbated_idx(starting)

        candidates = []

        #generate the candidates
        for _ in range(max_population):
            #shuffle the indexes
            random.shuffle(idx)

            #get topN items
            cand_idx = idx[:step]

            #generate the candidate
            cand = list(starting)

            for index in sorted(cand_idx, reverse=True):
                del cand[index]
            cand = ''.join(cand)

            #save the result
            candidates.append(cand)

        return candidates

    def adversarial_generation(self, sentence, max_population = 1, step = 100, max_it = -1, min_score = .05):
        """ Main routine to generate the adversarial samples """
        #poison the original sentence with the maximum perturbation

        #the following duplicated lines are essential to allow sentence == sanitize(x_adv)
        sentence = ' '.join(word_tokenize(sentence))
        sentence = ' '.join(word_tokenize(sentence))

        poisoned_sentence = self.poison(sentence)
        print(f"Starting perturbation number: {self.count_perturbations(poisoned_sentence)}")

        #compute the ground truth
        x_cnt = self.sanitize(poisoned_sentence)
        y = self.summarize([sentence, poisoned_sentence, x_cnt])
        print(sentence == x_cnt)

        y_true_summ = y[0]
        y_adv_summ = y[1]
        y_adv_cnt = y[2]
        score = self.score_function(y_true_summ, [y_adv_summ]) #upper bound of the DoS attack
        score_cnt = self.score_function(y_true_summ, [y_adv_cnt])
        print(f"Starting score: {score[0]:.4f}\tCONTROL: {score_cnt[0]:.4f}")
        assert score_cnt[0] == 1.0

        #init the best candidate -- at the beginning it is the original sentence
        best_candidate = (poisoned_sentence, y_adv_summ, score[0])

        execute = True
        it = 0

        #get the starting time
        start = datetime.datetime.now()

        while execute:
            it += 1 #increase the steps

            if max_it > 0:
                if it == max_it:
                    execute = False

            #generate candidates
            population = self.population_generation(best_candidate[0], max_population = 20, step = step)

            #summarize the population
            population_summ = self.summarize(population)

            #scoring function
            population_score = self.score_function(y_true_summ, population_summ)

            #we now need to find the best candidate
            # we first convert the population in a list of tuple with all the info
            population_log = []
            for xx, yy, zz in zip(population, population_summ, population_score):
                population_log.append((xx, yy, zz))

            #we now order such a list ascending
            population_log.sort(key = lambda x:x[2])

            #get the best candidate
            population_best_cand = population_log[0]

            # print(f"\nIteration {it + 1} executed in:\t{end - start}")
            print(f"\t--->best candidate score: {population_best_cand[2]:.4f}\t Number of perturbations: {self.count_perturbations(population_best_cand[0])[0]}")

            #check if we can improve or if we reached a local / global minimum
            if population_best_cand[2] <= min_score: #improvement
                best_candidate = population_best_cand
            else:
                execute = False

        #get the ending time
        end = datetime.datetime.now()
        print(f"\nExecuted in --- steps: {it}\ttime:\t{end - start}")
        print(f"Best candidate perturbation: {self.count_perturbations(best_candidate[0])}")

        return best_candidate, y_true_summ



if __name__ == '__main__':
    #get execution device
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(device)

    #max size
    max_size = 500

    #load the dataset
    dataset = load_dataset('cnn_dailymail', '3.0.0', split='test')
    # print("length:", len(dataset))
    # print(dataset[0])

    ARTICLES = []
    i = 0
    while len(ARTICLES) < max_size:
        if len(dataset[i]['article']) <= 2000:
            ARTICLES.append(dataset[i]['article'])
        i+=1
    #
    # print(np.average([len(x) for x in ARTICLES]))
    # print(np.max([len(x) for x in ARTICLES]))

    summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device = 0)

    # define the adversarial class
    adv_process = adversarial_summarizer(model = summarizer)

    for idx, article in enumerate(ARTICLES):
        print(f"ARTICLE: {idx}\tLength: {len(article)}")

        filename = f"./results/deletion/{idx}.json"

        if os.path.exists(filename):
            print("\f The file has been already generated")
        else:
            res, y_true_summ = adv_process.adversarial_generation(article)

            var = {
                'x_true': article,
                'y_true_summ': y_true_summ,
                'x_adv': res[0],
                'y_adv_summ': res[1],
                'score': res[2]
            }


            with open(filename, "w") as file:
                json.dump(var, file)
                file.close()
